{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YbMjcMlccDS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoxY8p0JLneH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8wVKuxacWmT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# load the dataset and the tokenizers\n",
        "data = load_dataset('magicsword/train-en-zh')['train']\n",
        "print(data['text'][0])\n",
        "tokenizer_en = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "tokenizer_zh = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
        "print(\"English vocab:\")\n",
        "print('length: ',len(tokenizer_en.vocab))\n",
        "print('special tokens: ',tokenizer_en.all_special_tokens)\n",
        "print('special tokens ids: ',tokenizer_en.all_special_ids)\n",
        "print(\"Chinese vocab:\")\n",
        "print('length: ',len(tokenizer_zh.vocab))\n",
        "print('special tokens: ',tokenizer_zh.all_special_tokens)\n",
        "print('special tokens ids: ',tokenizer_zh.all_special_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asU_OMJXkamU"
      },
      "outputs": [],
      "source": [
        "def map_fn(example):\n",
        "  return {'English':[t.split('|||')[0] for t in example['text']],\n",
        "          'Chinese':[t.split('|||')[1] for t in example['text']]}\n",
        "\n",
        "data = data.map(map_fn,batched=True)\n",
        "data = data.remove_columns(['text'])\n",
        "data['English'][:3],data['Chinese'][:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNnRx4FonEGO"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn_en(example):\n",
        "  return {'tokenized_English':[tokenizer_en.tokenize(en,add_special_tokens=True) for en in example['English']]}\n",
        "def tokenize_fn_zh(example):\n",
        "  return {'tokenized_Chinese':[tokenizer_zh.tokenize(zh,add_special_tokens=True) for zh in example['Chinese']]}\n",
        "\n",
        "data = data.map(tokenize_fn_en,batched=True)\n",
        "data = data.map(tokenize_fn_zh,batched=True)\n",
        "data = data.remove_columns(['English','Chinese'])\n",
        "print(data['tokenized_English'][14])\n",
        "print(data['tokenized_Chinese'][14])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydkkG3AAoJDv"
      },
      "outputs": [],
      "source": [
        "def tokenize_id_en(example):\n",
        "  return {'tokenized_en_id':[tokenizer_en.convert_tokens_to_ids(en) for en in example['tokenized_English']]}\n",
        "def tokenize_id_zh(example):\n",
        "  return {'tokenized_zh_id':[tokenizer_zh.convert_tokens_to_ids(zh) for zh in example['tokenized_Chinese']]}\n",
        "\n",
        "data = data.map(tokenize_id_en,batched=True)\n",
        "data = data.map(tokenize_id_zh,batched=True)\n",
        "print(data['tokenized_en_id'][14])\n",
        "print(data['tokenized_zh_id'][14])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['tokenized_en_id'][15])\n",
        "print(data['tokenized_zh_id'][15])"
      ],
      "metadata": {
        "id": "oqZkoPPf4FWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['tokenized_en_id'][2])\n",
        "print(data['tokenized_zh_id'][2])"
      ],
      "metadata": {
        "id": "qgrybUy54JrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0Y5ebaQJzRp"
      },
      "outputs": [],
      "source": [
        "tokenizer_en.decode(data['tokenized_en_id'][1]),tokenizer_zh.decode(data['tokenized_zh_id'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nmeo3D5AKifq"
      },
      "outputs": [],
      "source": [
        "data.to_json('/content/drive/MyDrive/2024Spring/641NaturalLanguageProcessing/NewFolder/dataset.json')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}