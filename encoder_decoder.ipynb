{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1HhWAQWNSdV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C0tSgULsrgS",
        "outputId": "668fa901-412b-4055-a168-21caf7125e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "tokenizer_zh = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
        "\n",
        "# hyperparameters\n",
        "# model\n",
        "vocab_size_en = len(tokenizer_en.vocab)\n",
        "vocab_size_zh = len(tokenizer_zh.vocab)\n",
        "print(vocab_size_en,vocab_size_zh)\n",
        "max_length = 512        # max length of the input sequence\n",
        "n_emb = 512             # embedding size\n",
        "n_head = 8             # number of heads in multi-head attention\n",
        "head_size = 64          # number of 'features' output by a single-head self-attention\n",
        "n_blocks = 3            # number of blocks in a encoder or decoder\n",
        "n_hidden = 1024\n",
        "assert head_size*n_head == n_emb, ''\n",
        "\n",
        "# training\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "learning_rate = 8e-5\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "9k1fsIS2ozDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class embedding(nn.Module):\n",
        "  def __init__(self,vocab_size,n_emb,max_len):\n",
        "    super().__init__()\n",
        "    self.n_emb = n_emb\n",
        "\n",
        "    self.word_embedding = nn.Embedding(vocab_size,n_emb)\n",
        "\n",
        "    pe = torch.zeros(max_len, n_emb)\n",
        "    position = torch.unsqueeze(torch.arange(0, max_len, dtype=torch.float),dim=1)\n",
        "    div_term = torch.exp(torch.arange(0, n_emb, 2).float() * (-math.log(10000.0) / n_emb))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self,x):\n",
        "    word_emb = self.word_embedding(x) * math.sqrt(self.n_emb)         # [B,T,n_emb]\n",
        "    pos_emb = self.pe[:,:word_emb.size(1),:]                           # [T,n_emb]\n",
        "    return pos_emb + word_emb                                         # [B,T,n_emb]"
      ],
      "metadata": {
        "id": "CorHNPMITXCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # test embedding\n",
        "# x = torch.randint(low=0,high=20,size=(2,5))\n",
        "# emb = embedding(vocab_size_en,n_emb,max_length)\n",
        "# out = emb(x)\n",
        "# out.shape"
      ],
      "metadata": {
        "id": "ppkCmfFDVH0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28386cf-d344-4207-cfa4-e5a185e4584e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 8])\n",
            "torch.Size([1, 32, 8])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchTransformer(nn.Module):\n",
        "  def __init__(self,n_emb,head_size,n_head,n_blocks,vocab_size_enc,vocab_size_dec,n_hidden,max_len):\n",
        "    super().__init__()\n",
        "    self.embedding_enc = embedding(vocab_size_enc,n_emb,max_len)\n",
        "    self.embedding_dec = embedding(vocab_size_dec,n_emb,max_len)\n",
        "    self.transformer = nn.Transformer(d_model=n_emb,nhead=n_head,num_encoder_layers=n_blocks,num_decoder_layers=n_blocks,dim_feedforward=n_hidden,batch_first=True)\n",
        "    self.linear = nn.Linear(n_emb,vocab_size_dec)\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def forward(self,seq_enc,seq_dec,mask_enc=None,mask_dec=None,mask_enc_padding=None,mask_dec_padding=None,memory_key_padding_mask=None):\n",
        "    emb_enc = self.embedding_enc(seq_enc)\n",
        "    emb_dec = self.embedding_dec(seq_dec)\n",
        "    out = self.transformer(src=emb_enc,tgt=emb_dec,\n",
        "                           src_mask=mask_enc,tgt_mask=mask_dec,\n",
        "                           src_key_padding_mask=mask_enc_padding,tgt_key_padding_mask=mask_dec_padding,\n",
        "                           memory_key_padding_mask=memory_key_padding_mask)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "\n",
        "  def encode(self,seq_enc,attention_mask_input,attention_mask_input_padding):\n",
        "    emb_enc = self.embedding_enc(seq_enc)\n",
        "    return self.transformer.encoder(emb_enc,attention_mask_input,attention_mask_input_padding)\n",
        "\n",
        "  def decode(self,seq_dec,memory,mask_dec):\n",
        "    emb_dec = self.embedding_dec(seq_dec)\n",
        "    return self.transformer.decoder(emb_dec,memory,mask_dec)\n",
        "\n",
        "  def generate(self,input_ids,attention_mask_input,attention_mask_input_padding,max_length=max_length,device=device):\n",
        "    input_ids = input_ids.to(device)\n",
        "    memory = self.encode(input_ids,attention_mask_input,attention_mask_input_padding).to(device)\n",
        "\n",
        "    output_token = [101]\n",
        "    while len(output_token)<=max_length:\n",
        "      inputs = output_token\n",
        "      seq_dec = torch.tensor(inputs).unsqueeze(0).to(device)\n",
        "      mask_dec = (torch.tril(torch.ones([seq_dec.shape[1],seq_dec.shape[1]]))==0).to(device)\n",
        "      logits = self.linear(self.decode(seq_dec,memory,mask_dec)[:,-1,:])\n",
        "      probs = F.softmax(logits,dim=-1)\n",
        "      temp = torch.argmax(probs,dim=-1).squeeze().item()\n",
        "      output_token.append(temp)\n",
        "      if temp == 102:\n",
        "        break\n",
        "    return output_token"
      ],
      "metadata": {
        "id": "mxfBcdPrEHrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # test TorchTransformer\n",
        "# import spacy\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# def generate(input_seq,test=False):\n",
        "#   tokens = nlp(input_seq)\n",
        "#   tokens = [stoi_en['<SOS>']] + [stoi_en[token.text.lower()] for token in tokens] + [stoi_en['<EOS>']]\n",
        "#   seq_enc = torch.tensor(tokens).to(device)\n",
        "#   seq_dec = torch.tensor([stoi_zh['<SOS>']]).to(device)\n",
        "#   output = model.generate(seq_enc,seq_dec,test)\n",
        "#   output = [itos_zh[o.item()] for o in output][1:]\n",
        "#   return ''.join(output)\n",
        "\n",
        "# model = TorchTransformer(n_emb,head_size,n_head,n_blocks,vocab_size_en,vocab_size_zh,n_hidden,max_length)\n",
        "# input_seq = \"harry potter\"\n",
        "# output_seq = generate(input_seq,test=True)"
      ],
      "metadata": {
        "id": "TDQ0hWcEdrP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}